{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# code by Tae Hwan Jung @graykode\n",
        "# Reference : https://github.com/hunkim/PyTorchZeroToAll/blob/master/14_2_seq2seq_att.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import psycopg2 as pg\n",
        "import pandas as pd\n",
        "\n",
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "class DatabaseConnection:\n",
        "    def __init__(self):\n",
        "        self.conn = self.connect()\n",
        "    def connect(self):\n",
        "        try:\n",
        "            self.conn = pg.connect(\n",
        "                \"dbname='stonksdb' user='postgres' host='localhost' password='admin'\")\n",
        "            print(\"Connection made succ\")\n",
        "            return self.conn\n",
        "        except:\n",
        "            print(\"I am unable to connect to the database\")\n",
        "            return None\n",
        "    def close(self):\n",
        "        self.conn.close()\n",
        "    \n",
        "    def GetConnector(self):\n",
        "        return self.conn\n",
        "    ### send query to database\n",
        "    def query(self, query):\n",
        "        cur = self.conn.cursor()\n",
        "        cur.execute(query)\n",
        "        return cur.fetchall()\n",
        "\n",
        "class DatasetAccess:\n",
        "    def __init__(self):\n",
        "        self.conn = DatabaseConnection()\n",
        "    def getNcompanies(self, N):\n",
        "        AllCompanies = self.conn.query(\"SELECT * FROM dataset limit \"+ str(N)+\"\")\n",
        "        return AllCompanies\n",
        "    def getStockFromSymbol(self, StockSymbol, column = '*'):\n",
        "        company = self.conn.query(\"SELECT * FROM dataset WHERE symbol = '\" + StockSymbol + \"'\")\n",
        "        self.getStockFromCompany(company, column)\n",
        "        return company\n",
        "    def getStockFromCompany(self, companies, column = '*'):\n",
        "        result = []\n",
        "        for company in companies:\n",
        "            result.append(self.conn.query(\"SELECT \"+self.convertListToString(column)+\" FROM stock WHERE identifier = '\" + str(company[0]) + \"'\"))\n",
        "        return result\n",
        "    def convertListToString(self, column):\n",
        "        if type(column) != list:\n",
        "            return column\n",
        "        result = ''\n",
        "        for item in column:\n",
        "            result += item + ', '\n",
        "        return result[:-2]\n",
        "    def getStockDFFromCompany(self, companies, column = '*'):\n",
        "        result = []\n",
        "        for company in companies:\n",
        "            result.append(pd.read_sql(\"SELECT \"+self.convertListToString(column)+\" FROM stock WHERE identifier = '\" + str(company[0]) + \"'\", self.conn.GetConnector()))\n",
        "        return result\n",
        "    def GetAllStocksAsDF(self):\n",
        "        PandaStock = pd.read_sql('SELECT * FROM stock', self.conn.GetConnector())\n",
        "        print(PandaStock)\n",
        "    \n",
        "def GetSingleStockDF():\n",
        "    dbAccess = DatasetAccess()\n",
        "    comp = dbAccess.getNcompanies(2)\n",
        "    return dbAccess.getStockDFFromCompany(comp, column=\"close\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "____________-\n",
            "____________-\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jeppe\\AppData\\Local\\Temp\\ipykernel_612\\2894743647.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
            "  return torch.FloatTensor(replica_Input), torch.FloatTensor(replica_Output), torch.FloatTensor(replica_Target)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([[[0.9900, 0.9904, 0.9912, 0.9904, 0.9908, 0.9896, 0.9908, 0.9904,\n",
              "           0.9896, 0.9896, 0.9900, 0.9888, 0.9896, 0.9892, 0.9892, 0.9884,\n",
              "           0.9888, 0.9884, 0.9828, 0.9812, 0.9804, 0.9792, 0.9796, 0.9800,\n",
              "           0.9792, 0.9788, 0.9788, 0.9792, 0.9792, 0.9796, 0.9788, 0.9800,\n",
              "           0.9792, 0.9792, 0.9796, 0.9784, 0.9788, 0.9788, 0.9800, 0.9792,\n",
              "           0.9788, 0.9788, 0.9792, 0.9780, 0.9776, 0.9776, 0.9760, 0.9768,\n",
              "           0.9772, 0.9764],\n",
              "          [0.9832, 0.9836, 0.9840, 0.9840, 0.9824, 0.9832, 0.9832, 0.9840,\n",
              "           0.9844, 0.9848, 0.9848, 0.9844, 0.9860, 0.9868, 0.9872, 0.9864,\n",
              "           0.9852, 0.9848, 0.9844, 0.9856, 0.9856, 0.9860, 0.9864, 0.9848,\n",
              "           0.9852, 0.9860, 0.9864, 0.9868, 0.9868, 0.9880, 0.9888, 0.9892,\n",
              "           0.9884, 0.9880, 0.9884, 0.9880, 0.9884, 0.9888, 0.9884, 0.9896,\n",
              "           0.9892, 0.9896, 0.9900, 0.9896, 0.9916, 0.9916, 0.9908, 0.9900,\n",
              "           0.9904, 0.9912],\n",
              "          [0.9739, 0.9731, 0.9739, 0.9739, 0.9739, 0.9747, 0.9747, 0.9760,\n",
              "           0.9756, 0.9760, 0.9752, 0.9739, 0.9739, 0.9727, 0.9723, 0.9731,\n",
              "           0.9739, 0.9747, 0.9747, 0.9731, 0.9747, 0.9739, 0.9747, 0.9764,\n",
              "           0.9764, 0.9768, 0.9768, 0.9764, 0.9764, 0.9764, 0.9768, 0.9764,\n",
              "           0.9772, 0.9788, 0.9784, 0.9776, 0.9776, 0.9768, 0.9764, 0.9768,\n",
              "           0.9764, 0.9768, 0.9768, 0.9768, 0.9764, 0.9768, 0.9764, 0.9764,\n",
              "           0.9764, 0.9743],\n",
              "          [0.9916, 0.9912, 0.9932, 0.9932, 0.9924, 0.9928, 0.9928, 0.9932,\n",
              "           0.9932, 0.9932, 0.9936, 0.9936, 0.9944, 0.9944, 0.9944, 0.9948,\n",
              "           0.9944, 0.9944, 0.9960, 0.9944, 0.9940, 0.9944, 0.9944, 0.9960,\n",
              "           0.9960, 0.9956, 0.9956, 0.9968, 0.9960, 0.9956, 0.9952, 0.9952,\n",
              "           0.9952, 0.9952, 0.9956, 0.9960, 0.9956, 0.9960, 0.9956, 0.9952,\n",
              "           0.9964, 0.9960, 0.9948, 0.9952, 0.9964, 0.9964, 0.9960, 0.9960,\n",
              "           0.9976, 0.9976],\n",
              "          [0.9972, 0.9988, 0.9980, 0.9976, 0.9980, 0.9980, 0.9984, 0.9980,\n",
              "           0.9964, 0.9964, 0.9984, 0.9968, 0.9960, 0.9968, 0.9964, 0.9972,\n",
              "           0.9968, 0.9952, 0.9960, 0.9940, 0.9936, 0.9928, 0.9928, 0.9924,\n",
              "           0.9908, 0.9908, 0.9896, 0.9904, 0.9916, 0.9924, 0.9900, 0.9912,\n",
              "           0.9920, 0.9916, 0.9924, 0.9924, 0.9916, 0.9932, 0.9932, 0.9944,\n",
              "           0.9956, 0.9948, 0.9948, 0.9964, 0.9960, 0.9944, 0.9952, 0.9964,\n",
              "           0.9952, 0.9956],\n",
              "          [0.9792, 0.9788, 0.9816, 0.9816, 0.9808, 0.9808, 0.9804, 0.9800,\n",
              "           0.9796, 0.9800, 0.9804, 0.9816, 0.9816, 0.9824, 0.9816, 0.9780,\n",
              "           0.9780, 0.9776, 0.9796, 0.9780, 0.9772, 0.9788, 0.9788, 0.9784,\n",
              "           0.9808, 0.9800, 0.9796, 0.9808, 0.9804, 0.9820, 0.9820, 0.9836,\n",
              "           0.9852, 0.9852, 0.9844, 0.9868, 0.9864, 0.9852, 0.9844, 0.9860,\n",
              "           0.9860, 0.9860, 0.9824, 0.9824, 0.9816, 0.9812, 0.9812, 0.9812,\n",
              "           0.9812, 0.9808],\n",
              "          [0.9860, 0.9844, 0.9844, 0.9852, 0.9856, 0.9856, 0.9856, 0.9868,\n",
              "           0.9876, 0.9872, 0.9868, 0.9860, 0.9900, 0.9900, 0.9892, 0.9904,\n",
              "           0.9896, 0.9896, 0.9896, 0.9900, 0.9892, 0.9900, 0.9900, 0.9896,\n",
              "           0.9892, 0.9892, 0.9908, 0.9916, 0.9904, 0.9912, 0.9912, 0.9904,\n",
              "           0.9900, 0.9892, 0.9900, 0.9900, 0.9896, 0.9904, 0.9908, 0.9900,\n",
              "           0.9912, 0.9920, 0.9916, 0.9916, 0.9908, 0.9900, 0.9900, 0.9892,\n",
              "           0.9888, 0.9888],\n",
              "          [0.9928, 0.9932, 0.9936, 0.9940, 0.9936, 0.9936, 0.9932, 0.9932,\n",
              "           0.9936, 0.9948, 0.9956, 0.9936, 0.9952, 0.9948, 0.9948, 0.9948,\n",
              "           0.9948, 0.9948, 0.9948, 0.9944, 0.9944, 0.9936, 0.9924, 0.9924,\n",
              "           0.9916, 0.9916, 0.9920, 0.9924, 0.9924, 0.9916, 0.9920, 0.9912,\n",
              "           0.9916, 0.9912, 0.9908, 0.9912, 0.9916, 0.9924, 0.9916, 0.9916,\n",
              "           0.9924, 0.9936, 0.9940, 0.9932, 0.9932, 0.9936, 0.9928, 0.9924,\n",
              "           0.9924, 0.9916],\n",
              "          [0.9595, 0.9603, 0.9591, 0.9595, 0.9603, 0.9607, 0.9603, 0.9611,\n",
              "           0.9615, 0.9599, 0.9595, 0.9583, 0.9595, 0.9603, 0.9631, 0.9631,\n",
              "           0.9627, 0.9623, 0.9627, 0.9627, 0.9623, 0.9603, 0.9603, 0.9603,\n",
              "           0.9607, 0.9615, 0.9619, 0.9623, 0.9627, 0.9627, 0.9623, 0.9615,\n",
              "           0.9615, 0.9595, 0.9599, 0.9615, 0.9619, 0.9611, 0.9607, 0.9611,\n",
              "           0.9611, 0.9619, 0.9615, 0.9611, 0.9615, 0.9615, 0.9615, 0.9615,\n",
              "           0.9619, 0.9603],\n",
              "          [0.9631, 0.9643, 0.9643, 0.9631, 0.9647, 0.9647, 0.9659, 0.9651,\n",
              "           0.9655, 0.9659, 0.9655, 0.9647, 0.9651, 0.9639, 0.9643, 0.9643,\n",
              "           0.9655, 0.9655, 0.9647, 0.9647, 0.9663, 0.9679, 0.9671, 0.9691,\n",
              "           0.9675, 0.9675, 0.9691, 0.9695, 0.9683, 0.9687, 0.9683, 0.9687,\n",
              "           0.9687, 0.9675, 0.9667, 0.9659, 0.9663, 0.9667, 0.9667, 0.9679,\n",
              "           0.9663, 0.9655, 0.9655, 0.9659, 0.9655, 0.9667, 0.9671, 0.9675,\n",
              "           0.9675, 0.9679]]]),\n",
              " tensor([[[0.9764, 0.9768, 0.9776, 0.9776, 0.9772, 0.9764, 0.9788, 0.9784,\n",
              "           0.9796, 0.9804, 0.9812, 0.9808, 0.9820, 0.9816, 0.9812, 0.9808,\n",
              "           0.9812, 0.9804, 0.9804, 0.9812, 0.9808, 0.9812, 0.9816, 0.9828,\n",
              "           0.9824, 0.9828, 0.9820, 0.9820, 0.9820, 0.9832, 0.9812, 0.9820,\n",
              "           0.9820, 0.9812, 0.9816, 0.9816, 0.9820, 0.9808, 0.9816, 0.9816,\n",
              "           0.9816, 0.9820, 0.9828, 0.9832, 0.9828, 0.9828, 0.9828, 0.9824,\n",
              "           0.9828, 0.9824],\n",
              "          [0.9908, 0.9928, 0.9920, 0.9924, 0.9816, 0.9824, 0.9844, 0.9836,\n",
              "           0.9840, 0.9796, 0.9788, 0.9792, 0.9812, 0.9816, 0.9828, 0.9780,\n",
              "           0.9784, 0.9788, 0.9752, 0.9788, 0.9780, 0.9764, 0.9772, 0.9764,\n",
              "           0.9764, 0.9747, 0.9780, 0.9784, 0.9788, 0.9792, 0.9788, 0.9796,\n",
              "           0.9780, 0.9772, 0.9760, 0.9756, 0.9756, 0.9760, 0.9731, 0.9719,\n",
              "           0.9735, 0.9735, 0.9723, 0.9743, 0.9727, 0.9727, 0.9723, 0.9735,\n",
              "           0.9760, 0.9756],\n",
              "          [0.9743, 0.9756, 0.9752, 0.9756, 0.9756, 0.9756, 0.9752, 0.9743,\n",
              "           0.9735, 0.9743, 0.9747, 0.9747, 0.9739, 0.9747, 0.9760, 0.9739,\n",
              "           0.9739, 0.9760, 0.9764, 0.9764, 0.9764, 0.9760, 0.9760, 0.9752,\n",
              "           0.9747, 0.9752, 0.9756, 0.9756, 0.9752, 0.9760, 0.9760, 0.9760,\n",
              "           0.9756, 0.9747, 0.9747, 0.9747, 0.9752, 0.9756, 0.9768, 0.9756,\n",
              "           0.9752, 0.9756, 0.9756, 0.9760, 0.9752, 0.9756, 0.9756, 0.9764,\n",
              "           0.9920, 0.9920],\n",
              "          [0.9972, 0.9972, 0.9976, 0.9976, 0.9984, 0.9984, 0.9988, 0.9988,\n",
              "           0.9992, 0.9988, 0.9992, 0.9988, 0.9992, 0.9992, 0.9996, 0.9984,\n",
              "           0.9984, 0.9980, 0.9980, 0.9980, 0.9984, 0.9992, 0.9992, 0.9992,\n",
              "           0.9996, 0.9996, 1.0000, 0.9988, 0.9984, 0.9976, 0.9976, 0.9968,\n",
              "           0.9968, 0.9964, 0.9960, 0.9952, 0.9960, 0.9952, 0.9960, 0.9952,\n",
              "           0.9956, 0.9952, 0.9956, 0.9952, 0.9952, 0.9956, 0.9956, 0.9960,\n",
              "           0.9968, 0.9972],\n",
              "          [0.9948, 0.9960, 0.9956, 0.9960, 0.9972, 0.9976, 0.9968, 0.9964,\n",
              "           0.9964, 0.9968, 0.9972, 0.9964, 0.9968, 0.9972, 0.9972, 0.9968,\n",
              "           0.9964, 0.9968, 0.9968, 0.9972, 0.9976, 0.9968, 0.9972, 0.9960,\n",
              "           0.9952, 0.9936, 0.9940, 0.9944, 0.9948, 0.9856, 0.9856, 0.9880,\n",
              "           0.9852, 0.9840, 0.9828, 0.9832, 0.9808, 0.9796, 0.9792, 0.9792,\n",
              "           0.9768, 0.9776, 0.9772, 0.9784, 0.9776, 0.9784, 0.9776, 0.9780,\n",
              "           0.9776, 0.9788],\n",
              "          [0.9816, 0.9820, 0.9828, 0.9824, 0.9816, 0.9824, 0.9828, 0.9840,\n",
              "           0.9828, 0.9824, 0.9824, 0.9820, 0.9816, 0.9804, 0.9804, 0.9812,\n",
              "           0.9808, 0.9812, 0.9804, 0.9796, 0.9796, 0.9796, 0.9796, 0.9804,\n",
              "           0.9800, 0.9808, 0.9820, 0.9828, 0.9832, 0.9836, 0.9840, 0.9848,\n",
              "           0.9852, 0.9856, 0.9856, 0.9860, 0.9848, 0.9844, 0.9848, 0.9856,\n",
              "           0.9856, 0.9856, 0.9856, 0.9852, 0.9848, 0.9856, 0.9856, 0.9856,\n",
              "           0.9856, 0.9864],\n",
              "          [0.9876, 0.9868, 0.9868, 0.9880, 0.9872, 0.9876, 0.9880, 0.9880,\n",
              "           0.9880, 0.9888, 0.9888, 0.9896, 0.9892, 0.9888, 0.9888, 0.9896,\n",
              "           0.9896, 0.9900, 0.9880, 0.9880, 0.9884, 0.9884, 0.9892, 0.9884,\n",
              "           0.9884, 0.9876, 0.9880, 0.9880, 0.9872, 0.9880, 0.9884, 0.9888,\n",
              "           0.9888, 0.9888, 0.9892, 0.9896, 0.9892, 0.9904, 0.9900, 0.9908,\n",
              "           0.9912, 0.9912, 0.9908, 0.9912, 0.9908, 0.9908, 0.9916, 0.9916,\n",
              "           0.9924, 0.9928],\n",
              "          [0.9916, 0.9916, 0.9920, 0.9912, 0.9912, 0.9916, 0.9916, 0.9920,\n",
              "           0.9916, 0.9912, 0.9908, 0.9912, 0.9912, 0.9916, 0.9912, 0.9920,\n",
              "           0.9904, 0.9896, 0.9904, 0.9900, 0.9691, 0.9687, 0.9683, 0.9687,\n",
              "           0.9683, 0.9667, 0.9675, 0.9667, 0.9675, 0.9679, 0.9687, 0.9683,\n",
              "           0.9675, 0.9667, 0.9623, 0.9603, 0.9623, 0.9619, 0.9619, 0.9615,\n",
              "           0.9619, 0.9627, 0.9627, 0.9627, 0.9631, 0.9643, 0.9635, 0.9639,\n",
              "           0.9631, 0.9611],\n",
              "          [0.9599, 0.9603, 0.9595, 0.9599, 0.9607, 0.9603, 0.9619, 0.9615,\n",
              "           0.9619, 0.9599, 0.9603, 0.9607, 0.9611, 0.9607, 0.9607, 0.9607,\n",
              "           0.9611, 0.9611, 0.9619, 0.9623, 0.9611, 0.9603, 0.9603, 0.9603,\n",
              "           0.9603, 0.9603, 0.9607, 0.9607, 0.9607, 0.9591, 0.9587, 0.9607,\n",
              "           0.9619, 0.9611, 0.9623, 0.9631, 0.9655, 0.9659, 0.9663, 0.9667,\n",
              "           0.9643, 0.9639, 0.9631, 0.9623, 0.9631, 0.9635, 0.9639, 0.9639,\n",
              "           0.9639, 0.9635],\n",
              "          [0.9667, 0.9675, 0.9671, 0.9675, 0.9667, 0.9663, 0.9659, 0.9667,\n",
              "           0.9663, 0.9655, 0.9667, 0.9679, 0.9675, 0.9679, 0.9683, 0.9683,\n",
              "           0.9675, 0.9671, 0.9671, 0.9671, 0.9667, 0.9727, 0.9695, 0.9683,\n",
              "           0.9707, 0.9743, 0.9768, 0.9784, 0.9808, 0.9804, 0.9820, 0.9824,\n",
              "           0.9828, 0.9836, 0.9852, 0.9860, 0.9872, 0.9880, 0.9892, 0.9896,\n",
              "           0.9892, 0.9868, 0.9872, 0.9852, 0.9848, 0.9836, 0.9812, 0.9828,\n",
              "           0.9848, 0.9836]]]),\n",
              " tensor([[[0.9764, 0.9768, 0.9776, 0.9776, 0.9772, 0.9764, 0.9788, 0.9784,\n",
              "           0.9796, 0.9804, 0.9812, 0.9808, 0.9820, 0.9816, 0.9812, 0.9808,\n",
              "           0.9812, 0.9804, 0.9804, 0.9812, 0.9808, 0.9812, 0.9816, 0.9828,\n",
              "           0.9824, 0.9828, 0.9820, 0.9820, 0.9820, 0.9832, 0.9812, 0.9820,\n",
              "           0.9820, 0.9812, 0.9816, 0.9816, 0.9820, 0.9808, 0.9816, 0.9816,\n",
              "           0.9816, 0.9820, 0.9828, 0.9832, 0.9828, 0.9828, 0.9828, 0.9824,\n",
              "           0.9828, 0.9824],\n",
              "          [0.9908, 0.9928, 0.9920, 0.9924, 0.9816, 0.9824, 0.9844, 0.9836,\n",
              "           0.9840, 0.9796, 0.9788, 0.9792, 0.9812, 0.9816, 0.9828, 0.9780,\n",
              "           0.9784, 0.9788, 0.9752, 0.9788, 0.9780, 0.9764, 0.9772, 0.9764,\n",
              "           0.9764, 0.9747, 0.9780, 0.9784, 0.9788, 0.9792, 0.9788, 0.9796,\n",
              "           0.9780, 0.9772, 0.9760, 0.9756, 0.9756, 0.9760, 0.9731, 0.9719,\n",
              "           0.9735, 0.9735, 0.9723, 0.9743, 0.9727, 0.9727, 0.9723, 0.9735,\n",
              "           0.9760, 0.9756],\n",
              "          [0.9743, 0.9756, 0.9752, 0.9756, 0.9756, 0.9756, 0.9752, 0.9743,\n",
              "           0.9735, 0.9743, 0.9747, 0.9747, 0.9739, 0.9747, 0.9760, 0.9739,\n",
              "           0.9739, 0.9760, 0.9764, 0.9764, 0.9764, 0.9760, 0.9760, 0.9752,\n",
              "           0.9747, 0.9752, 0.9756, 0.9756, 0.9752, 0.9760, 0.9760, 0.9760,\n",
              "           0.9756, 0.9747, 0.9747, 0.9747, 0.9752, 0.9756, 0.9768, 0.9756,\n",
              "           0.9752, 0.9756, 0.9756, 0.9760, 0.9752, 0.9756, 0.9756, 0.9764,\n",
              "           0.9920, 0.9920],\n",
              "          [0.9972, 0.9972, 0.9976, 0.9976, 0.9984, 0.9984, 0.9988, 0.9988,\n",
              "           0.9992, 0.9988, 0.9992, 0.9988, 0.9992, 0.9992, 0.9996, 0.9984,\n",
              "           0.9984, 0.9980, 0.9980, 0.9980, 0.9984, 0.9992, 0.9992, 0.9992,\n",
              "           0.9996, 0.9996, 1.0000, 0.9988, 0.9984, 0.9976, 0.9976, 0.9968,\n",
              "           0.9968, 0.9964, 0.9960, 0.9952, 0.9960, 0.9952, 0.9960, 0.9952,\n",
              "           0.9956, 0.9952, 0.9956, 0.9952, 0.9952, 0.9956, 0.9956, 0.9960,\n",
              "           0.9968, 0.9972],\n",
              "          [0.9948, 0.9960, 0.9956, 0.9960, 0.9972, 0.9976, 0.9968, 0.9964,\n",
              "           0.9964, 0.9968, 0.9972, 0.9964, 0.9968, 0.9972, 0.9972, 0.9968,\n",
              "           0.9964, 0.9968, 0.9968, 0.9972, 0.9976, 0.9968, 0.9972, 0.9960,\n",
              "           0.9952, 0.9936, 0.9940, 0.9944, 0.9948, 0.9856, 0.9856, 0.9880,\n",
              "           0.9852, 0.9840, 0.9828, 0.9832, 0.9808, 0.9796, 0.9792, 0.9792,\n",
              "           0.9768, 0.9776, 0.9772, 0.9784, 0.9776, 0.9784, 0.9776, 0.9780,\n",
              "           0.9776, 0.9788],\n",
              "          [0.9816, 0.9820, 0.9828, 0.9824, 0.9816, 0.9824, 0.9828, 0.9840,\n",
              "           0.9828, 0.9824, 0.9824, 0.9820, 0.9816, 0.9804, 0.9804, 0.9812,\n",
              "           0.9808, 0.9812, 0.9804, 0.9796, 0.9796, 0.9796, 0.9796, 0.9804,\n",
              "           0.9800, 0.9808, 0.9820, 0.9828, 0.9832, 0.9836, 0.9840, 0.9848,\n",
              "           0.9852, 0.9856, 0.9856, 0.9860, 0.9848, 0.9844, 0.9848, 0.9856,\n",
              "           0.9856, 0.9856, 0.9856, 0.9852, 0.9848, 0.9856, 0.9856, 0.9856,\n",
              "           0.9856, 0.9864],\n",
              "          [0.9876, 0.9868, 0.9868, 0.9880, 0.9872, 0.9876, 0.9880, 0.9880,\n",
              "           0.9880, 0.9888, 0.9888, 0.9896, 0.9892, 0.9888, 0.9888, 0.9896,\n",
              "           0.9896, 0.9900, 0.9880, 0.9880, 0.9884, 0.9884, 0.9892, 0.9884,\n",
              "           0.9884, 0.9876, 0.9880, 0.9880, 0.9872, 0.9880, 0.9884, 0.9888,\n",
              "           0.9888, 0.9888, 0.9892, 0.9896, 0.9892, 0.9904, 0.9900, 0.9908,\n",
              "           0.9912, 0.9912, 0.9908, 0.9912, 0.9908, 0.9908, 0.9916, 0.9916,\n",
              "           0.9924, 0.9928],\n",
              "          [0.9916, 0.9916, 0.9920, 0.9912, 0.9912, 0.9916, 0.9916, 0.9920,\n",
              "           0.9916, 0.9912, 0.9908, 0.9912, 0.9912, 0.9916, 0.9912, 0.9920,\n",
              "           0.9904, 0.9896, 0.9904, 0.9900, 0.9691, 0.9687, 0.9683, 0.9687,\n",
              "           0.9683, 0.9667, 0.9675, 0.9667, 0.9675, 0.9679, 0.9687, 0.9683,\n",
              "           0.9675, 0.9667, 0.9623, 0.9603, 0.9623, 0.9619, 0.9619, 0.9615,\n",
              "           0.9619, 0.9627, 0.9627, 0.9627, 0.9631, 0.9643, 0.9635, 0.9639,\n",
              "           0.9631, 0.9611],\n",
              "          [0.9599, 0.9603, 0.9595, 0.9599, 0.9607, 0.9603, 0.9619, 0.9615,\n",
              "           0.9619, 0.9599, 0.9603, 0.9607, 0.9611, 0.9607, 0.9607, 0.9607,\n",
              "           0.9611, 0.9611, 0.9619, 0.9623, 0.9611, 0.9603, 0.9603, 0.9603,\n",
              "           0.9603, 0.9603, 0.9607, 0.9607, 0.9607, 0.9591, 0.9587, 0.9607,\n",
              "           0.9619, 0.9611, 0.9623, 0.9631, 0.9655, 0.9659, 0.9663, 0.9667,\n",
              "           0.9643, 0.9639, 0.9631, 0.9623, 0.9631, 0.9635, 0.9639, 0.9639,\n",
              "           0.9639, 0.9635],\n",
              "          [0.9667, 0.9675, 0.9671, 0.9675, 0.9667, 0.9663, 0.9659, 0.9667,\n",
              "           0.9663, 0.9655, 0.9667, 0.9679, 0.9675, 0.9679, 0.9683, 0.9683,\n",
              "           0.9675, 0.9671, 0.9671, 0.9671, 0.9667, 0.9727, 0.9695, 0.9683,\n",
              "           0.9707, 0.9743, 0.9768, 0.9784, 0.9808, 0.9804, 0.9820, 0.9824,\n",
              "           0.9828, 0.9836, 0.9852, 0.9860, 0.9872, 0.9880, 0.9892, 0.9896,\n",
              "           0.9892, 0.9868, 0.9872, 0.9852, 0.9848, 0.9836, 0.9812, 0.9828,\n",
              "           0.9848, 0.9836]]]))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def make_batch():\n",
        "    replica_input = train\n",
        "    replica_input = [np.array([replica_input])]\n",
        "    replica_Input = [n for n in replica_input[0]]\n",
        "    print(\"____________-\")\n",
        "    replica_output = target\n",
        "    replica_output = [np.array([replica_output])]\n",
        "    replica_Output = [n for n in replica_output[0]]\n",
        "    print(\"____________-\")\n",
        "    replica_target = target\n",
        "    replica_target = [np.array([replica_target])]\n",
        "    replica_Target = [n for n in replica_target[0]]\n",
        "    \n",
        "    #input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]] # takes the number and one hot encodes it\n",
        "    #print(input_batch)\n",
        "    #print(replica_Output)\n",
        "    #output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]] # takes the number and one hot encodes it\n",
        "    #target_batch = [[word_dict[n] for n in sentences[2].split()]] # takes the number\n",
        "    # print(input_batch)\n",
        "    # print([np.array([replica_input])])\n",
        "    # make tensor\n",
        "    # return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
        "    return torch.FloatTensor(replica_Input), torch.FloatTensor(replica_Output), torch.FloatTensor(replica_Target)\n",
        "\n",
        "make_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "\n",
        "        # Linear for attention\n",
        "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
        "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
        "\n",
        "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
        "        enc_inputs = enc_inputs.transpose(0, 1)  # enc_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "        dec_inputs = dec_inputs.transpose(0, 1)  # dec_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "\n",
        "        # enc_outputs : [n_step, batch_size, num_directions(=1) * n_hidden], matrix F\n",
        "        # enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
        "\n",
        "        trained_attn = []\n",
        "        hidden = enc_hidden\n",
        "        n_step = len(dec_inputs)\n",
        "        model = torch.empty([n_step, 1, n_class])\n",
        "\n",
        "        for i in range(n_step):  # each time step\n",
        "            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n",
        "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
        "            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n",
        "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
        "\n",
        "            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n",
        "            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
        "            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n",
        "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
        "\n",
        "        # make model shape [n_step, n_class]\n",
        "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
        "\n",
        "    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one 'dec_output' with 'enc_outputs'\n",
        "        n_step = len(enc_outputs)\n",
        "        attn_scores = torch.zeros(n_step)  # attn_scores : [n_step]\n",
        "\n",
        "        for i in range(n_step):\n",
        "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
        "\n",
        "        # Normalize scores to weights in range 0 to 1\n",
        "        return F.softmax(attn_scores).view(1, 1, -1)\n",
        "\n",
        "    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n",
        "        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n",
        "        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connection made succ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jeppe\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\sql.py:758: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
            "  warnings.warn(\n",
            "C:\\Users\\Jeppe\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\sql.py:758: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
            "  warnings.warn(\n",
            "C:\\Users\\Jeppe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "C:\\Users\\Jeppe\\AppData\\Local\\Temp\\ipykernel_612\\938175066.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(attn_scores).view(1, 1, -1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "____________-\n",
            "____________-\n",
            "Epoch: 0400 cost = 21645.035156\n",
            "Epoch: 0800 cost = 20399.583984\n",
            "Epoch: 1200 cost = 28746.265625\n",
            "Epoch: 1600 cost = 27468.175781\n",
            "Epoch: 2000 cost = 28559.324219\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    n_step = 5 # number of cells(= number of Step)\n",
        "    n_hidden = 128 # number of hidden units in one cell\n",
        "    closingData = GetSingleStockDF()[1][:1000]\n",
        "    n_class = 50\n",
        "    chunks = 10\n",
        "\n",
        "    closingData = np.array(closingData).flatten()\n",
        "    closingData = closingData / closingData.max()\n",
        "    data = np.array_split(closingData, chunks)\n",
        "\n",
        "    train = [d[:50] for d in data]\n",
        "    target = [d[50:] for d in data]\n",
        "    \n",
        "    hidden = torch.zeros(1, 1, n_hidden)\n",
        "\n",
        "    model = Attention()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    input_batch, output_batch, target_batch = make_batch()\n",
        "    \n",
        "    # Train\n",
        "    for epoch in range(2000):\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(input_batch, hidden, output_batch)\n",
        "\n",
        "        loss = criterion(output, target_batch.squeeze(0))\n",
        "        if (epoch + 1) % 400 == 0:\n",
        "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
