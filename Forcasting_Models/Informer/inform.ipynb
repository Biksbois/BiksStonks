{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Informer model\n",
    "- The components and how it is made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "from datetime import datetime\n",
    "from data.data_loader import Dataset_Custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding of the raw timeseries\n",
    "- --> into a representation with temporal encoding embedded into the representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '.'\n",
    "flag = 'train'  \n",
    "size = [72, 48, 24] # seq_len, lbl_len, pred_len \n",
    "features = 'MS' # Multivariate feautures IN, Single OUT \n",
    "data_path = 'dbank_h.csv'\n",
    "target = 'close'\n",
    "scale = True \n",
    "inverse = False\n",
    "timenc = 0 \n",
    "freq = 'h'\n",
    "train_data = Dataset_Custom(root_path, flag, size, features, data_path, \n",
    "                                          target, scale, inverse, timenc, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72, 5), (72, 5), (72, 4), (72, 4))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data is processed by window sliding across it\n",
    "# x will have seq_len length sequence for each of the columns in this case ohlc + vol\n",
    "# y will have the last lbl_len of seq_len + pred_len \n",
    "x, y, x_mark, y_mark = train_data[0]\n",
    "x.shape, y.shape, x_mark.shape, y_mark.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  9,  0, 15],\n",
       "       [ 3, 10,  1,  8],\n",
       "       [ 3, 10,  1,  9]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_mark and y_mark are the the timestamp info\n",
    "# for timenc = 0, no encoding is done, so will have: [month, day, weekday, hour]\n",
    "x_mark[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15217391, -0.5       , -0.23333333, -0.31369863],\n",
       "       [-0.15217391, -0.33333333, -0.2       , -0.3109589 ],\n",
       "       [-0.10869565, -0.33333333, -0.2       , -0.3109589 ]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting timenc = 1 --> 'timeF' will encode time between -0.5 to 0.5 \n",
    "train_data_enc = Dataset_Custom(root_path, flag, size, features, data_path, \n",
    "                                          target, scale, inverse, 1, freq)\n",
    "x, y, x_mark_e, y_mark_e = train_data_enc[0]\n",
    "x_mark_e[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1522, -0.5000, -0.2333, -0.4945],\n",
       "        [-0.1522, -0.3333, -0.2000, -0.4945],\n",
       "        [-0.1087, -0.3333, -0.2000, -0.4945]], dtype=torch.float64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again because freq = 'h' we can see in utils.features.timefeatures \n",
    "# that the time features are encoded as: [Hour of day, day of week, day of month, day of year]\n",
    "# it is encoded to be between -0.5 to 0.5 by following computations:\n",
    "hourOfDay = lambda x: (x[3]/23.0) - 0.5\n",
    "dayOfWeek = lambda x: (x[2]/6) - 0.5\n",
    "dayOfMonth = lambda x: ((x[1] - 1)/30.0) - 0.5\n",
    "DayOfYear = lambda x: ((x[0] - 1)/ 365) - 0.5 # this is not correct\n",
    "encode_time = lambda x: torch.tensor([hourOfDay(x), dayOfWeek(x), dayOfMonth(x), DayOfYear(x)])\n",
    "torch.vstack([encode_time(x_mark[i]) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1522, -0.5000, -0.2333, -0.3137],\n",
       "        [-0.1522, -0.3333, -0.2000, -0.3110],\n",
       "        [-0.1087, -0.3333, -0.2000, -0.3110]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct way\n",
    "# to get features as day of year we need to use some pandas tools\n",
    "convert_to_datetime = lambda x: datetime(year=2020, month=x[0], day=x[1], hour=x[3])\n",
    "dates = pd.to_datetime([convert_to_datetime(x) for x in x_mark])\n",
    "offset = to_offset('h')\n",
    "hourOfDay_ = lambda x: (x.hour/23.0) - 0.5\n",
    "dayOfWeek_ = lambda x: (x.dayofweek/6.0) - 0.5\n",
    "dayOfMonth_ = lambda x: ((x.day - 1)/30.0) - 0.5\n",
    "DayOfYearCorrect = lambda x: ((x.dayofyear - 1)/ 365) - 0.5\n",
    "# trying again now \n",
    "encode_time_correct = lambda x: torch.tensor([hourOfDay_(x), dayOfWeek_(x), dayOfMonth_(x), DayOfYearCorrect(x)])\n",
    "torch.vstack([encode_time_correct(dates[i]) for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataEmbedding from models.embed\n",
    "Forward (x, x_mark)\n",
    "\n",
    "    - Return: dropout(value_embedding(x) + position_embedding(x) + temporal_embedding(x_mark))\n",
    "\n",
    "\n",
    "- we explore each of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_in = x.shape[1] # number of features in input\n",
    "d_model = 512 # dimension of the model, matches with n_heads = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### value_embedding = models.embed.TokenEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbot",
   "language": "python",
   "name": "tbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
